{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is K-Nearest Neighbors (KNN) and How Does It Work in Both Classification and Regression Problems?\n",
        "\n",
        "**Answer:**  \n",
        "**K-Nearest Neighbors (KNN)** is a **supervised learning algorithm** used for both **classification** and **regression** tasks. It is a **non-parametric** and **instance-based** learning method, meaning it doesnâ€™t make assumptions about the data distribution and makes predictions based on the closest training samples.\n",
        "\n",
        "---\n",
        "\n",
        "## How KNN Works\n",
        "1. Choose the number of neighbors, **K** (e.g., K = 3 or 5).  \n",
        "2. For a given test point:\n",
        "   - Calculate the **distance** between the test point and all training points (commonly using **Euclidean distance**).  \n",
        "   - Identify the **K nearest neighbors** based on these distances.  \n",
        "3. Depending on the task:\n",
        "   - **Classification:** The class most common among the K neighbors is assigned to the test point (majority voting).  \n",
        "   - **Regression:** The output is the **average** (or weighted average) of the values of the K neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "## KNN for Classification\n",
        "- Example: Predicting whether a loan applicant will **default or not** based on features like income, age, and credit score.  \n",
        "- The algorithm finds the K most similar applicants and predicts the class with the majority vote.  \n",
        "\n",
        "**Prediction Rule:**  \n",
        "\\[\n",
        "\\hat{y} = \\text{mode}(y_i \\text{ of K nearest neighbors})\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## KNN for Regression\n",
        "- Example: Predicting **house prices** based on nearby housesâ€™ prices.  \n",
        "- The algorithm takes the **average** of the prices of the K nearest houses.  \n",
        "\n",
        "**Prediction Rule:**  \n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{K} \\sum_{i=1}^{K} y_i\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## Key Points\n",
        "- **K value selection:**  \n",
        "  - Small K â†’ more sensitive to noise (overfitting).  \n",
        "  - Large K â†’ smoother decision boundary (may underfit).  \n",
        "- **Distance metrics:** Euclidean, Manhattan, or Minkowski distance are commonly used.  \n",
        "- **Feature scaling:** Important, since KNN depends on distance â€” features must be normalized or standardized.\n"
      ],
      "metadata": {
        "id": "wiNMVpft0VWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the Curse of Dimensionality and How Does It Affect KNN Performance?\n",
        "\n",
        "**Answer:**  \n",
        "The **Curse of Dimensionality** refers to the problems that arise when data has a **large number of features (dimensions)**. As the number of dimensions increases, the data becomes **sparse**, distances between points become **less meaningful**, and models that rely on distance measuresâ€”like **K-Nearest Neighbors (KNN)**â€”suffer in performance.\n",
        "\n",
        "---\n",
        "\n",
        "## What Happens in High Dimensions\n",
        "1. **Data Sparsity:**  \n",
        "   - In high-dimensional spaces, data points are far apart and the notion of â€œclosenessâ€ becomes weak.  \n",
        "   - Most data points appear to be at almost the same distance from each other.  \n",
        "\n",
        "2. **Distance Measures Lose Meaning:**  \n",
        "   - In high dimensions, the difference between the nearest and farthest neighbors becomes very small.  \n",
        "   - This makes it difficult for KNN to find truly â€œnearestâ€ neighbors.\n",
        "\n",
        "3. **Increased Computation:**  \n",
        "   - More features mean more distance calculations, which increases computational cost and memory usage.\n",
        "\n",
        "---\n",
        "\n",
        "## Impact on KNN Performance\n",
        "- **Reduced Accuracy:**  \n",
        "  - Since KNN relies heavily on distance to classify or predict, less meaningful distances lead to poor model decisions.  \n",
        "- **Overfitting Risk:**  \n",
        "  - With too many irrelevant or noisy features, KNN may misclassify because every point starts looking equally close.  \n",
        "- **Slower Predictions:**  \n",
        "  - KNN must compute distances to all training samples, and high dimensionality increases this burden.\n",
        "\n",
        "---\n",
        "\n",
        "## How to Handle the Curse of Dimensionality\n",
        "1. **Feature Selection:**  \n",
        "   - Keep only the most relevant features using techniques like correlation analysis or feature importance scores.  \n",
        "2. **Dimensionality Reduction:**  \n",
        "   - Apply **PCA (Principal Component Analysis)** or **t-SNE** to reduce the number of dimensions.  \n",
        "3. **Normalization:**  \n",
        "   - Scale features so that all contribute equally to the distance metric."
      ],
      "metadata": {
        "id": "KcIX0x3O0gwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is Principal Component Analysis (PCA)? How is it Different from Feature Selection?\n",
        "\n",
        "**Answer:**  \n",
        "**Principal Component Analysis (PCA)** is an **unsupervised dimensionality reduction technique** used to transform high-dimensional data into a smaller set of **uncorrelated features** called **principal components**. These components capture the **maximum variance** (information) present in the original dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## How PCA Works\n",
        "1. **Standardize the Data:**  \n",
        "   - Ensure all features have the same scale (mean = 0, variance = 1).\n",
        "\n",
        "2. **Compute the Covariance Matrix:**  \n",
        "   - Measures how features vary with respect to each other.\n",
        "\n",
        "3. **Calculate Eigenvalues and Eigenvectors:**  \n",
        "   - Eigenvectors represent directions (principal components).  \n",
        "   - Eigenvalues represent the amount of variance captured by each component.\n",
        "\n",
        "4. **Select Top Components:**  \n",
        "   - Choose the top *k* components that capture most of the variance (e.g., 95%).\n",
        "\n",
        "5. **Transform the Data:**  \n",
        "   - Project original data onto the new principal components to obtain reduced dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## Example\n",
        "If you have 10 features, PCA might find that 2 or 3 principal components capture 95% of the information â€” allowing you to reduce dimensions while preserving most of the variability.\n",
        "\n",
        "---\n",
        "\n",
        "## PCA vs. Feature Selection\n",
        "\n",
        "| Aspect | PCA (Feature Extraction) | Feature Selection |\n",
        "|--------|--------------------------|-------------------|\n",
        "| **Approach** | Creates **new features** by combining existing ones (linear combinations) | **Selects** a subset of the original features |\n",
        "| **Goal** | Reduce dimensionality while retaining maximum variance | Keep only the most relevant features for prediction |\n",
        "| **Output Features** | Transformed, uncorrelated (principal components) | Original features (subset) |\n",
        "| **Interpretability** | Less interpretable (new synthetic features) | More interpretable (original features retained) |\n",
        "| **Supervision** | Unsupervised method | Can be supervised or unsupervised |\n",
        "\n"
      ],
      "metadata": {
        "id": "_oSdGP1D0uAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Eigenvalues and Eigenvectors in PCA, and Why are They Important?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In **Principal Component Analysis (PCA)**, **eigenvalues** and **eigenvectors** are mathematical concepts derived from the **covariance matrix** of the data. They play a crucial role in identifying the **principal components**, which are the new directions along which the data varies the most.\n",
        "\n",
        "---\n",
        "\n",
        "##  What are Eigenvectors?\n",
        "- **Eigenvectors** represent the **directions (axes)** of the new feature space.\n",
        "- Each eigenvector defines a **principal component**, which is a linear combination of the original features.\n",
        "- They show **where** the data spreads the most in the multi-dimensional space.\n",
        "\n",
        "### Example:\n",
        "If your dataset has two correlated features, PCA will find new axes (eigenvectors) that point in the directions of maximum and minimum variance.\n",
        "\n",
        "---\n",
        "\n",
        "##  What are Eigenvalues?\n",
        "- **Eigenvalues** represent the **magnitude (amount)** of variance captured by each eigenvector.\n",
        "- A **larger eigenvalue** means that the corresponding eigenvector (principal component) explains **more variance** in the data.\n",
        "- Eigenvalues help us **rank** components and decide how many to keep.\n",
        "\n",
        "---\n",
        "\n",
        "##  Why Are They Important in PCA?\n",
        "\n",
        "| Concept | Role in PCA |\n",
        "|----------|--------------|\n",
        "| **Eigenvectors** | Determine the **direction** of maximum variance (principal components). |\n",
        "| **Eigenvalues** | Determine the **importance** (variance explained) of each component. |\n",
        "| **Dimensionality Reduction** | Components with small eigenvalues are often dropped since they carry little information. |\n",
        "\n",
        "---\n",
        "\n",
        "##  Example Analogy\n",
        "Imagine data points scattered on a 2D plane:\n",
        "- The **eigenvector** points along the direction where points are most spread out.\n",
        "- The **eigenvalue** tells **how much** spread (variance) there is in that direction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dN9vapSN1ZU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: How do KNN and PCA Complement Each Other When Applied in a Single Pipeline?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** and **Principal Component Analysis (PCA)** are often used together in machine learning workflows because they **complement each otherâ€™s strengths and limitations**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step-by-Step Relationship Between PCA and KNN\n",
        "\n",
        "### 1. **Dimensionality Reduction Before KNN**\n",
        "- KNN relies on **distance metrics** (like Euclidean distance) to classify or predict.\n",
        "- In high-dimensional data, distances become less meaningful â€” this is known as the **Curse of Dimensionality**.\n",
        "- **PCA reduces the number of dimensions** while keeping the most informative patterns, making KNN more efficient and accurate.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Noise Reduction**\n",
        "- PCA removes **correlated and noisy features** by focusing on directions of maximum variance.\n",
        "- KNN performs better on cleaner, less noisy data since its decisions depend directly on nearby points.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Improved Computation Efficiency**\n",
        "- KNNâ€™s computation cost grows with the number of features.\n",
        "- Using PCA to reduce features **speeds up distance calculations**, improving KNNâ€™s performance on large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Better Visualization and Interpretability**\n",
        "- After applying PCA, data can be projected into 2D or 3D space.\n",
        "- This helps visualize how KNN separates classes or clusters in reduced-dimensional space.\n",
        "\n",
        "\n",
        "\n",
        "# Create a pipeline with PCA + KNN\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with PCA + KNN:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mkUDl7qqAwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine Dataset with and without Feature Scaling. Compare Model Accuracy in Both Cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# KNN Without Feature Scaling\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "\n",
        "#  KNN With Feature Scaling\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "\n",
        "# Compare Results\n",
        "\n",
        "print(\"Accuracy without scaling :\", round(accuracy_no_scale, 4))\n",
        "print(\"Accuracy with scaling    :\", round(accuracy_scaled, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P30UmJ8tCp2W",
        "outputId": "f1345983-d534-45ad-a22b-cb9e7a794c7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling : 0.7407\n",
            "Accuracy with scaling    : 0.963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Train a PCA Model on the Wine Dataset and Print the Explained Variance Ratio of Each Principal Component\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 3: Display the explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance))],\n",
        "    'Explained Variance Ratio': explained_variance\n",
        "})\n",
        "\n",
        "print(df)\n",
        "print(\"\\nTotal Variance Explained:\", round(sum(explained_variance), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD1yxL7ZDG8h",
        "outputId": "f22067b8-76e2-491a-a7ec-18a995997307"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Principal Component  Explained Variance Ratio\n",
            "0                  PC1                  0.361988\n",
            "1                  PC2                  0.192075\n",
            "2                  PC3                  0.111236\n",
            "3                  PC4                  0.070690\n",
            "4                  PC5                  0.065633\n",
            "5                  PC6                  0.049358\n",
            "6                  PC7                  0.042387\n",
            "7                  PC8                  0.026807\n",
            "8                  PC9                  0.022222\n",
            "9                 PC10                  0.019300\n",
            "10                PC11                  0.017368\n",
            "11                PC12                  0.012982\n",
            "12                PC13                  0.007952\n",
            "\n",
            "Total Variance Explained: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-Transformed Dataset (Retain Top 2 Components). Compare the Accuracy with the Original Dataset.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Train KNN on the original (scaled) dataset\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# Step 3: Apply PCA (retain top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Step 4: Train KNN on PCA-transformed dataset\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Step 5: Compare results\n",
        "print(\"Accuracy on Original Scaled Data :\", round(accuracy_original, 4))\n",
        "print(\"Accuracy on PCA (2 Components)   :\", round(accuracy_pca, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uqGEDjBDwJk",
        "outputId": "994876df-e297-4b50-dba4-d65aaf19350e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Scaled Data : 0.963\n",
            "Accuracy on PCA (2 Components)   : 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with Different Distance Metrics (Euclidean, Manhattan) on the Scaled Wine Dataset and Compare the Results\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Train KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Step 3: Train KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Step 4: Compare results\n",
        "print(\"Accuracy using Euclidean Distance :\", round(accuracy_euclidean, 4))\n",
        "print(\"Accuracy using Manhattan Distance :\", round(accuracy_manhattan, 4))\n",
        "\n",
        "# Optional: Display confusion matrices for deeper insight\n",
        "print(\"\\nConfusion Matrix (Euclidean):\\n\", confusion_matrix(y_test, y_pred_euclidean))\n",
        "print(\"\\nConfusion Matrix (Manhattan):\\n\", confusion_matrix(y_test, y_pred_manhattan))\n",
        "\n",
        "# Classification reports\n",
        "print(\"\\nClassification Report (Euclidean):\\n\", classification_report(y_test, y_pred_euclidean))\n",
        "print(\"\\nClassification Report (Manhattan):\\n\", classification_report(y_test, y_pred_manhattan))"
      ],
      "metadata": {
        "id": "4Jcb7LncGqk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Using PCA and KNN for High-Dimensional Gene Expression Data\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "High-dimensional datasets, such as gene expression profiles, pose unique challenges in machine learning:\n",
        "\n",
        "- Thousands of gene features (dimensions) but only a few patient samples.  \n",
        "- Traditional models easily **overfit** due to sparse data in high-dimensional space.  \n",
        "- **KNN** alone may fail because distance metrics become less meaningful (**curse of dimensionality**).  \n",
        "\n",
        "To build a robust and interpretable pipeline, we can combine **PCA for dimensionality reduction** with **KNN for classification**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "**Principal Component Analysis (PCA)** transforms the original high-dimensional data into a new set of uncorrelated variables called **principal components (PCs)**.\n",
        "\n",
        "- Steps:\n",
        "  1. **Standardize the data**: Mean = 0, Variance = 1.\n",
        "  2. **Compute covariance matrix** and extract **eigenvectors** and **eigenvalues**.\n",
        "  3. **Transform data** onto the principal components.\n",
        "\n",
        "- Benefits for gene expression data:\n",
        "  - Captures most **variance** while discarding noise.  \n",
        "  - Reduces feature space, mitigating **overfitting**.  \n",
        "  - Improves computational efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Decide How Many Components to Keep\n",
        "\n",
        "- Use **explained variance ratio** to determine the number of PCs that capture most of the information.\n",
        "- Common strategy:\n",
        "  - Keep enough components to retain **90â€“95% of total variance**.\n",
        "  - Example:\n",
        "    ```python\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    num_components = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
        "    ```\n",
        "- This balances **information retention** with **dimensionality reduction**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "\n",
        "- KNN is **distance-based**; lower-dimensional data improves accuracy and reduces sensitivity to noise.\n",
        "- Steps:\n",
        "  1. Apply PCA transformation to both **training and test sets**.  \n",
        "  2. Train **KNN classifier** on the PCA-reduced features.  \n",
        "  3. Predict cancer types for test patients.\n",
        "\n",
        "- Hyperparameter considerations:\n",
        "  - **k (number of neighbors)**: Use cross-validation to choose optimal k.  \n",
        "  - **Distance metric**: Euclidean distance works well after scaling.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Evaluate the Model\n",
        "\n",
        "- Use **k-fold cross-validation** (e.g., 5- or 10-fold) to estimate performance.\n",
        "- Metrics suitable for biomedical classification:\n",
        "  - **Accuracy**: Overall correctness.  \n",
        "  - **Precision, Recall, F1-Score**: Important for imbalanced cancer classes.  \n",
        "  - **ROC-AUC**: Measures discrimination ability of the model.\n",
        "\n",
        "- Example evaluation in Python:\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(knn, X_pca, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validated Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "zn13ut-GHn6Q"
      }
    }
  ]
}