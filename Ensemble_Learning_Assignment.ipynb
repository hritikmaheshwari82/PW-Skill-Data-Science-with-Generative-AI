{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRd7f65SegoyrgNJnKJmdL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ensemble Learning | Assignment"],"metadata":{"id":"zABeU6ehFcNZ"}},{"cell_type":"markdown","source":["**Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n","\n","\n","**Answer:**  \n","**Ensemble Learning** is a machine learning technique that combines the predictions of multiple individual models (called *base learners* or *weak learners*) to create a more accurate and robust final model. The idea is that by aggregating the strengths of several models, the ensemble can outperform any single model alone.\n","\n","## Key Idea Behind Ensemble Learning\n","The key idea is that **a group of weak models, when combined properly, can produce a strong model**. Each individual model may make some errors, but when their predictions are aggregated (for example, by averaging or voting), these errors can cancel out, leading to improved overall accuracy and generalization.\n","\n","## Types of Ensemble Methods\n","1. **Bagging (Bootstrap Aggregating)**  \n","   - Trains multiple models on different random subsets of the training data.  \n","   - Example: **Random Forest**  \n","   - Reduces variance and prevents overfitting.\n","\n","2. **Boosting**  \n","   - Trains models sequentially, where each new model focuses on correcting the errors of the previous ones.  \n","   - Example: **AdaBoost, Gradient Boosting, XGBoost**  \n","   - Reduces bias and improves model accuracy.\n","\n","3. **Stacking**  \n","   - Combines multiple base models’ predictions using a **meta-model** (another model that learns how to best combine them).  \n","   - Example: Using Logistic Regression as a meta-model to combine Decision Tree and SVM outputs.\n"],"metadata":{"id":"naJQRzk5FlO_"}},{"cell_type":"markdown","source":["**Question 2: What is the difference between Bagging and Boosting?**\n","\n","**Answer:**  \n","Bagging and Boosting are both ensemble learning techniques, but they differ in how they build and combine the base models.\n","\n","| Feature | Bagging | Boosting |\n","|---------|---------|----------|\n","| **Full Form** | Bootstrap Aggregating | N/A |\n","| **Objective** | Reduces **variance** and prevents overfitting | Reduces **bias** and improves accuracy |\n","| **How Models Are Trained** | Models are trained **independently** on different random subsets of data | Models are trained **sequentially**, where each new model focuses on the errors of previous models |\n","| **Data Sampling** | Random sampling **with replacement** (bootstrap) | All data points are used, but weights are adjusted to focus on misclassified examples |\n","| **Prediction Combination** | **Voting** (for classification) or **averaging** (for regression) | **Weighted combination** based on model performance |\n","| **Example Algorithms** | Random Forest | AdaBoost, Gradient Boosting, XGBoost |\n","| **Error Handling** | Reduces **variance** by averaging errors | Reduces **bias** by learning from mistakes |\n","\n"],"metadata":{"id":"yDe8Egn_GoB2"}},{"cell_type":"markdown","source":["**Question 3: What is Bootstrap Sampling and What Role Does It Play in Bagging Methods Like Random Forest?**\n","\n","**Answer:**  \n","**Bootstrap Sampling** is a statistical technique where multiple random samples are drawn **with replacement** from the original dataset. Each sample is of the same size as the original dataset, but because of sampling with replacement, some data points may appear multiple times while others may be left out.\n","\n","## Role of Bootstrap Sampling in Bagging (e.g., Random Forest)\n","1. **Creating Diverse Training Sets:**  \n","   - Each base model (like a decision tree in Random Forest) is trained on a different bootstrap sample.  \n","   - This introduces **diversity** among the models, which is crucial for reducing variance.\n","\n","2. **Reducing Overfitting:**  \n","   - Because each tree sees a slightly different dataset, Random Forest avoids overfitting to the original dataset.  \n","\n","3. **Enabling Out-of-Bag (OOB) Error Estimation:**  \n","   - Data points not included in a bootstrap sample (called *out-of-bag samples*) can be used to estimate the model’s performance without needing a separate test set.\n"],"metadata":{"id":"ulTiDsrpHBNr"}},{"cell_type":"markdown","source":["**Question 4: What are Out-of-Bag (OOB) Samples and How is OOB Score Used to Evaluate Ensemble Models?**\n","\n","**Answer:**  \n","**Out-of-Bag (OOB) samples** are the data points **not included** in a bootstrap sample when training a base model in Bagging methods (like Random Forest). Since each bootstrap sample leaves out about **one-third of the original data**, these left-out samples can be used for evaluation.\n","\n","## Role of OOB Samples\n","1. **Evaluation Without a Separate Test Set:**  \n","   - OOB samples act as a **built-in validation set**, allowing estimation of model performance without needing an external test dataset.\n","\n","2. **OOB Score Calculation:**  \n","   - Each data point that is OOB for some trees is predicted using only those trees.  \n","   - The predictions are aggregated (majority vote for classification, averaging for regression) to compute the **OOB score**, which reflects model accuracy or error.\n","\n","3. **Advantages:**  \n","   - Saves data since no separate test set is needed.  \n","   - Provides an unbiased estimate of model generalization performance.  \n","   - Helps in **hyperparameter tuning** and **feature importance estimation**."],"metadata":{"id":"XZGwEAkVHSqd"}},{"cell_type":"markdown","source":["**Question 5: Compare Feature Importance Analysis in a Single Decision Tree vs. a Random Forest**\n","\n","**Answer:**  \n","Feature importance measures how much each feature contributes to the predictive performance of a model. Both Decision Trees and Random Forests provide feature importance, but the way it is computed and interpreted differs.\n","\n","| Feature | Decision Tree | Random Forest |\n","|---------|---------------|---------------|\n","| **Calculation** | Importance is based on how much a feature **reduces impurity** (like Gini or entropy) in the tree’s splits | Importance is averaged over **all trees** in the forest, reducing bias from a single tree |\n","| **Stability** | Can be **unstable**, as small changes in data can lead to different splits and importance scores | More **stable and reliable** because it aggregates importance across multiple trees |\n","| **Bias Towards Certain Features** | Can be **biased toward features with more levels** (categorical) or continuous features with many split points | Less biased than a single tree due to averaging over multiple diverse trees |\n","| **Interpretation** | Shows importance for the **specific tree** | Shows overall importance for the **entire ensemble**, providing better generalization |\n","| **Use Case** | Good for quick analysis or simple datasets | Preferred for robust feature selection in complex datasets |\n"],"metadata":{"id":"qyb_Cm24HikK"}},{"cell_type":"code","source":["# Question 6: Random Forest Feature Importance on Breast Cancer Dataset\n","\n","# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","import numpy as np\n","\n","# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","feature_names = data.feature_names\n","\n","# Train a Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Get feature importance scores\n","importances = rf.feature_importances_\n","\n","# Create a DataFrame for better visualization\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance': importances\n","})\n","\n","# Sort features by importance\n","feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n","\n","# Print the top 5 most important features\n","top5_features = feature_importance_df.head(5)\n","print(\"Top 5 Most Important Features:\\n\")\n","print(top5_features)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4F_Gm952ILrQ","executionInfo":{"status":"ok","timestamp":1760703929158,"user_tz":-330,"elapsed":12605,"user":{"displayName":"Hritik Maheshwari","userId":"09252595404223873834"}},"outputId":"c4ad9d37-c1f0-4d58-fbc6-932e52313179"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features:\n","\n","                 Feature  Importance\n","23            worst area    0.139357\n","27  worst concave points    0.132225\n","7    mean concave points    0.107046\n","20          worst radius    0.082848\n","22       worst perimeter    0.080850\n"]}]},{"cell_type":"code","source":["# Question 7: Bagging Classifier vs Single Decision Tree on Iris Dataset\n","\n","# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a single Decision Tree\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train, y_train)\n","y_pred_dt = dt.predict(X_test)\n","accuracy_dt = accuracy_score(y_test, y_pred_dt)\n","\n","# Train a Bagging Classifier using Decision Trees\n","bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n","                            random_state=42, n_jobs=-1)  # <-- changed base_estimator to estimator\n","bagging.fit(X_train, y_train)\n","y_pred_bagging = bagging.predict(X_test)\n","accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n","\n","# Print the accuracies\n","print(f\"Accuracy of Single Decision Tree: {accuracy_dt:.4f}\")\n","print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6geeiEriJBfv","executionInfo":{"status":"ok","timestamp":1760704106729,"user_tz":-330,"elapsed":135,"user":{"displayName":"Hritik Maheshwari","userId":"09252595404223873834"}},"outputId":"257f00c7-e3de-4fd1-a0df-115e683dc144"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Single Decision Tree: 1.0000\n","Accuracy of Bagging Classifier: 1.0000\n"]}]},{"cell_type":"code","source":["# Question 8: Random Forest Classifier with Hyperparameter Tuning using GridSearchCV\n","\n","# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the Random Forest Classifier\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Define hyperparameter grid\n","param_grid = {\n","    'n_estimators': [50, 100, 150],\n","    'max_depth': [None, 3, 5, 7]\n","}\n","\n","# Perform GridSearchCV\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters\n","best_params = grid_search.best_params_\n","print(f\"Best Hyperparameters: {best_params}\")\n","\n","# Evaluate the final model on the test set\n","best_rf = grid_search.best_estimator_\n","y_pred = best_rf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Final Accuracy on Test Set: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nh-IZnEkbein","executionInfo":{"status":"ok","timestamp":1760709018735,"user_tz":-330,"elapsed":27085,"user":{"displayName":"Hritik Maheshwari","userId":"09252595404223873834"}},"outputId":"47f0b24c-cc06-4b6c-baf1-22e7461c9c93"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n","Final Accuracy on Test Set: 1.0000\n"]}]},{"cell_type":"code","source":["# Question 9: Bagging Regressor vs Random Forest Regressor on California Housing Dataset\n","\n","# Import necessary libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load California Housing dataset\n","california = fetch_california_housing()\n","X = california.data\n","y = california.target\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a Bagging Regressor using Decision Trees\n","bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50,\n","                           random_state=42, n_jobs=-1)\n","bagging.fit(X_train, y_train)\n","y_pred_bagging = bagging.predict(X_test)\n","mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n","\n","# Train a Random Forest Regressor\n","rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","y_pred_rf = rf.predict(X_test)\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","\n","# Print Mean Squared Errors\n","print(f\"Mean Squared Error of Bagging Regressor: {mse_bagging:.4f}\")\n","print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cw9WR3mlcFac","executionInfo":{"status":"ok","timestamp":1760709100126,"user_tz":-330,"elapsed":22880,"user":{"displayName":"Hritik Maheshwari","userId":"09252595404223873834"}},"outputId":"2c11064d-9800-4417-b468-e2ea832af671"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error of Bagging Regressor: 0.2579\n","Mean Squared Error of Random Forest Regressor: 0.2565\n"]}]},{"cell_type":"markdown","source":["**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n","\n","**You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:**\n","\n"," **● Choose between Bagging or Boosting**\n","\n"," **● Handle overfitting**\n","\n"," **● Select base models**\n","\n"," **● Evaluate performance using cross-validation**\n","\n"," **● Justify how ensemble learning improves decision-making in this real-world context. bold text bold text**\n","\n","**Answer:**  \n","\n","When predicting loan default at a financial institution, ensemble techniques can improve predictive accuracy, stability, and generalization. Here’s a step-by-step approach:\n","\n","---\n","\n","## 1. Choosing Between Bagging or Boosting\n","- **Bagging** (e.g., Random Forest) is suitable when the base models are **high-variance**, like Decision Trees, and we want to **reduce overfitting**.  \n","- **Boosting** (e.g., AdaBoost, Gradient Boosting, XGBoost) is suitable when base models are **weak learners** and the goal is to **reduce bias** by sequentially correcting errors.  \n","- **Decision:** Start with **Random Forest (Bagging)** to get robust predictions. If higher accuracy is needed, use **Boosting** for fine-tuned error correction.\n","\n","---\n","\n","## 2. Handling Overfitting\n","- **Bagging:** Reduces variance by averaging multiple trees trained on different bootstrap samples.  \n","- **Boosting:** Control overfitting by tuning parameters like `learning_rate`, `n_estimators`, and `max_depth`.  \n","- **Additional techniques:**  \n","  - Feature selection to remove irrelevant variables.  \n","  - Regularization (e.g., L1/L2 penalties in boosting models).  \n","  - Cross-validation to monitor generalization.\n","\n","---\n","\n","## 3. Selecting Base Models\n","- **Decision Trees** are commonly used because they:  \n","  - Capture non-linear patterns in customer data.  \n","  - Work well with both bagging and boosting.  \n","- For tabular financial data, **shallow trees** (max_depth 3-5) are often good base learners for boosting.  \n","\n","---\n","\n","## 4. Evaluating Performance Using Cross-Validation\n","- Use **k-fold cross-validation** (e.g., k=5 or 10) to assess model performance.  \n","- Metrics to track:  \n","  - **Accuracy** (overall correctness)  \n","  - **Precision and Recall** (especially for detecting defaulters)  \n","  - **ROC-AUC** (for ranking risk probabilities)  \n","- Cross-validation ensures the model generalizes well to unseen customers and avoids overfitting.\n","\n","---\n","\n","## 5. How Ensemble Learning Improves Decision-Making\n","- **Higher predictive accuracy:** Combining multiple models reduces individual model errors.  \n","- **Stability:** Aggregating predictions reduces the impact of outliers or noisy data.  \n","- **Better risk assessment:** Helps the institution identify high-risk customers more reliably.  \n","- **Informed decisions:** Accurate predictions enable better loan approval policies, interest rate assignments, and proactive interventions for at-risk customers.  \n","\n","**Conclusion:**  \n","Using ensemble learning, such as Bagging or Boosting, allows financial institutions to build **robust, accurate, and reliable models** for predicting loan default, which ultimately improves risk management and decision-making.\n"],"metadata":{"id":"fEtfHlRXca8z"}}]}