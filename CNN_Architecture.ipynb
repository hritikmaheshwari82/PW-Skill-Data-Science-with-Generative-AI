{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "(Include your Python code and output in the code box below.)'\n",
        "\n",
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "KoEMNRqe4VV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture Assignment"
      ],
      "metadata": {
        "id": "xHwhn_yn-2Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "# --------------------------------------------------\n",
        "Filter (kernel): small learnable weight tensor that slides across the input and performs dot-products to detect local patterns (edges, textures, shapes, objects, …).  \n",
        "Feature map: 2-D (or 3-D) output produced by one filter; each value shows how strongly the corresponding spatial location matches the filter’s pattern.  \n",
        "Many filters = many feature maps = rich hierarchical representation that the network learns automatically.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "# --------------------------------------------------\n",
        "Padding: extra pixels (usually 0) added around the input before convolution.  \n",
        "  – “same” padding: output spatial size ≈ input size.  \n",
        "  – “valid” padding: no padding; output shrinks.  \n",
        "\n",
        "Stride: step size with which the filter slides.  \n",
        "  – stride = 1 ⇒ slide one pixel at a time (largest output).  \n",
        "  – stride > 1 ⇒ down-samples the output (smaller feature map).  \n",
        "\n",
        "Output spatial size formula (square case):  \n",
        "O = ⌊(I − K + 2P)/S⌋ + 1  \n",
        "I=input size, K=kernel size, P=padding, S=stride.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "# --------------------------------------------------\n",
        "Receptive field (RF): region of the original input image that can influence the activation of a single neuron in a given layer.  \n",
        "Early layers have small RF (few pixels); deeper layers have larger RF (hundreds of pixels) thanks to stacking convolutions and pooling.  \n",
        "Large RF is crucial for deep nets because it allows neurons to integrate global context and recognize large-scale objects.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "# --------------------------------------------------\n",
        "Parameter count in a conv layer = (F × F × C_in) × C_out + C_out(bias)  \n",
        "F = filter spatial size, C_in = input channels, C_out = output channels.  \n",
        "Stride does NOT change the number of parameters; it only changes output resolution.  \n",
        "Larger filters increase parameters quadratically in F; common remedy: stack multiple 3×3 filters to get large RF with fewer params.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "# --------------------------------------------------\n",
        "\n",
        "LeNet is one of the earliest CNNs with a shallow architecture containing around 5–7 layers. It uses relatively large filters (5×5) and was mainly designed for handwritten digit recognition. It has low computational requirements and performs well on simple datasets.\n",
        "\n",
        "AlexNet marked a major advancement in 2012, consisting of 8 layers with larger filters in the initial layers (11×11, 5×5). It introduced important innovations such as ReLU activation, dropout, and data augmentation. AlexNet significantly improved performance on the ImageNet dataset.\n",
        "\n",
        "VGG is much deeper, with 16 or 19 layers, and uses a uniform architecture of small 3×3 filters throughout the network. Although computationally expensive due to its large number of parameters, VGG achieves very high accuracy and is widely used as a feature extractor in many applications."
      ],
      "metadata": {
        "id": "JtygnDaS-gaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 2. Reshape (CNN requires 4D input)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# 3. Normalize\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# 4. One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 5. Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 6. Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 7. Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 8. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz7CGjxfHoIA",
        "outputId": "3dcc6c16-709c-4c20-95bc-45b4689376ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 55ms/step - accuracy: 0.8837 - loss: 0.3750 - val_accuracy: 0.9838 - val_loss: 0.0536\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 50ms/step - accuracy: 0.9826 - loss: 0.0563 - val_accuracy: 0.9858 - val_loss: 0.0491\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9890 - loss: 0.0370 - val_accuracy: 0.9880 - val_loss: 0.0421\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 50ms/step - accuracy: 0.9924 - loss: 0.0246 - val_accuracy: 0.9905 - val_loss: 0.0415\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 51ms/step - accuracy: 0.9931 - loss: 0.0204 - val_accuracy: 0.9890 - val_loss: 0.0404\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9869 - loss: 0.0468\n",
            "Test Accuracy: 0.9897000193595886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "# CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 2. Normalize pixel values (0–255 → 0–1)\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# 3. One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 4. Build CNN model for RGB images\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 5. Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6. Train model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 7. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy7r4UwsISry",
        "outputId": "c3d8285b-3ca2-48d3-9edd-e9c262a8af93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 148ms/step - accuracy: 0.3265 - loss: 1.8110 - val_accuracy: 0.5524 - val_loss: 1.2706\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 136ms/step - accuracy: 0.5547 - loss: 1.2533 - val_accuracy: 0.6478 - val_loss: 1.0249\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 139ms/step - accuracy: 0.6286 - loss: 1.0542 - val_accuracy: 0.6922 - val_loss: 0.8915\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 135ms/step - accuracy: 0.6759 - loss: 0.9276 - val_accuracy: 0.7112 - val_loss: 0.8451\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 136ms/step - accuracy: 0.7053 - loss: 0.8399 - val_accuracy: 0.7268 - val_loss: 0.8061\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 139ms/step - accuracy: 0.7411 - loss: 0.7398 - val_accuracy: 0.7462 - val_loss: 0.7584\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 135ms/step - accuracy: 0.7673 - loss: 0.6771 - val_accuracy: 0.7346 - val_loss: 0.7827\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 135ms/step - accuracy: 0.7782 - loss: 0.6372 - val_accuracy: 0.7520 - val_loss: 0.7446\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 136ms/step - accuracy: 0.7940 - loss: 0.5872 - val_accuracy: 0.7646 - val_loss: 0.7112\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 134ms/step - accuracy: 0.8150 - loss: 0.5250 - val_accuracy: 0.7630 - val_loss: 0.7098\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7580 - loss: 0.7270\n",
            "Test Accuracy: 0.7537999749183655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Device configuration (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Transformations (normalization + tensor conversion)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 3. Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# 4. Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 5. Define CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# 6. Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Accuracy evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MTMCIwUJGO1",
        "outputId": "76594ab5-9966-4cc0-f06b-c0677c73bca4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.09MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 172kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.53MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.42MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0368\n",
            "Epoch [2/5], Loss: 0.0064\n",
            "Epoch [3/5], Loss: 0.0296\n",
            "Epoch [4/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.1171\n",
            "Test Accuracy: 99.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 1: Create fake dataset folders (Normal / Pneumonia)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "base_path = \"/content/chest_xray_demo\"\n",
        "train_dir = os.path.join(base_path, \"train\")\n",
        "val_dir = os.path.join(base_path, \"val\")\n",
        "\n",
        "# Create folders\n",
        "for path in [train_dir, val_dir]:\n",
        "    os.makedirs(os.path.join(path, \"Normal\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, \"Pneumonia\"), exist_ok=True)\n",
        "\n",
        "# Generate random images for Normal & Pneumonia\n",
        "def create_random_images(folder, count):\n",
        "    for i in range(count):\n",
        "        arr = np.random.randint(0, 255, (128,128,3), dtype=np.uint8)\n",
        "        img = Image.fromarray(arr)\n",
        "        img.save(os.path.join(folder, f\"img{i}.jpg\"))\n",
        "\n",
        "# create random images for training\n",
        "create_random_images(os.path.join(train_dir, \"Normal\"), 20)\n",
        "create_random_images(os.path.join(train_dir, \"Pneumonia\"), 20)\n",
        "\n",
        "# create random images for validation\n",
        "create_random_images(os.path.join(val_dir, \"Normal\"), 5)\n",
        "create_random_images(os.path.join(val_dir, \"Pneumonia\"), 5)\n",
        "\n",
        "print(\"Demo dataset created successfully!\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 2: Preprocessing and Generators\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 3: Build CNN Model\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(train_gen.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 4: Train the model\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=3)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 5: Evaluation\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "loss, acc = model.evaluate(val_gen)\n",
        "print(\"Validation Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoOmK-5xJTIU",
        "outputId": "f5c69bd7-14b3-4ac4-b9d8-33ba24cd35df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo dataset created successfully!\n",
            "Found 40 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 372ms/step - accuracy: 0.4156 - loss: 4.5693 - val_accuracy: 0.5000 - val_loss: 0.7232\n",
            "Epoch 2/3\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - accuracy: 0.4406 - loss: 0.7571 - val_accuracy: 0.5000 - val_loss: 0.6919\n",
            "Epoch 3/3\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - accuracy: 0.6764 - loss: 0.6793 - val_accuracy: 0.5000 - val_loss: 0.6919\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6918\n",
            "Validation Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: End-to-end CNN medical X-ray classifier → Streamlit web app\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "1. Business / regulatory framing\n",
        "   - FDA-class II software → treat every step (data, model, code) as auditable.\n",
        "   - PHI compliance: de-identify DICOM, store on encrypted bucket, HIPAA-BAA cloud.\n",
        "\n",
        "2. Data acquisition & inventory\n",
        "   - Public sets: NIH ChestX-ray14, RSNA Pneumonia Challenge, CheXpert.\n",
        "   - Private hospital PACS export → DICOM → PNG 1 k², 8-bit, single view (PA).\n",
        "   - Target balance: ~10 k pneumonia, ~10 k normal after augmentation.\n",
        "\n",
        "3. Label consolidation\n",
        "   - Radiologist consensus reads (2+ board-cert.) → binary label.\n",
        "   - Uncertain / borderline cases excluded to keep 0/1 gold standard.\n",
        "\n",
        "4. Pre-processing pipeline (re-usable module)\n",
        "   a. DICOM → PNG converter (pydicom) + automatic windowing (lung window).\n",
        "   b. Auto-crop lung fields (pre-trained U-Net seg) → center square 512×512.\n",
        "   c. Intensity normalize to [0,1] via 1-st & 99-th percentile clipping.\n",
        "   d. Data augmentation (albumentations): shift/rotate 10°, brightness ±15%, horizontal flip, cutout 32×32.\n",
        "   e. 5-fold patient-wise stratified split (no patient in both train & test).\n",
        "\n",
        "5. Model design\n",
        "   - Backbone: ImageNet-pre-trained DenseNet-121 (good small-lesion sensitivity).\n",
        "   - Replace final classifier → GlobalAveragePooling → Dropout 0.3 → Dense 1 (sigmoid).\n",
        "   - Freeze first 120 layers for 3 epochs, then unfreeze entire net (lower LR).\n",
        "\n",
        "6. Training strategy\n",
        "   - Loss: binary-cross-entropy + 0.01 L2 on last layer.\n",
        "   - Optimizer: AdamW lr=1e-4, cosine decay to 1e-6, batch 16 (GPU memory).\n",
        "   - Callbacks: EarlyStopping(patience=5, monitor=val_AUC), ModelCheckpoint(save_best), ReduceLROnPlateau.\n",
        "   - Class imbalance: use pos_weight = (N_neg/N_pos) in BCE.\n",
        "   - 50 epochs ≈ 45 min on single V100.\n",
        "\n",
        "7. Evaluation & clinical metrics\n",
        "   - ROC-AUC, PR-AUC, sensitivity @ 95 % specificity, F1.\n",
        "   - Confidence interval via 1 000 bootstrap on patient level.\n",
        "   - Grad-CAM++ heat-maps reviewed by radiologist → qualitative safety check.\n",
        "\n",
        "8. Explainability artefact\n",
        "   - Save Grad-CAM overlay PNG alongside prediction for physician review.\n",
        "\n",
        "9. Model packaging\n",
        "   - Export best checkpoint to ONNX (fp16) → 40 MB.\n",
        "   - Convert to TensorFlow Lite or keep ONNX for onnx-runtime (fast CPU fallback).\n",
        "\n",
        "10. Web-app skeleton (Streamlit)\n",
        "    File tree:\n",
        "    app/\n",
        "     ├── requirements.txt\n",
        "     ├── main.py\n",
        "     ├── model/\n",
        "     │    └── chest_densenet121.onnx\n",
        "     └── utils/\n",
        "          ├── preprocess.py\n",
        "          ├── gradcam.py\n",
        "          └── infer.py\n",
        "\n",
        "    main.py excerpt:\n",
        "    ```python\n",
        "    import streamlit as st, onnxruntime as ort, cv2, numpy as np, requests, io\n",
        "    from utils import preprocess, gradcam, infer\n",
        "\n",
        "    st.set_page_config(page_title=\"Pneumonia Screening\")\n",
        "    st.title(\"Chest X-ray Pneumonia Classifier\")\n",
        "    uploaded = st.file_uploader(\"Upload chest X-ray (PNG/JPG/DICOM)\", type=[\"png\",\"jpg\",\"dcm\"])\n",
        "    if uploaded:\n",
        "        img = preprocess.load_and_clean(uploaded)          # step 4 a-c\n",
        "        prob = infer.predict(img)                          # ONNX, 60 ms CPU\n",
        "        st.metric(\"Pneumonia probability\", f\"{prob:.1%}\")\n",
        "        if prob > 0.5:\n",
        "            st.warning(\"Pneumonia suspected – refer for radiologist review.\")\n",
        "        else:\n",
        "            st.success(\"Normal study.\")\n",
        "        with st.expander(\"View heat-map\"):\n",
        "            heat = gradcam.compute(img)\n",
        "            st.image(heat, caption=\"Grad-CAM++\", use_column_width=True)\n",
        "    ```\n",
        "\n",
        "11. CI/CD & deployment\n",
        "    - GitHub Actions: lint → pytest → Docker build → push to AWS ECR.\n",
        "    - Infrastructure: ECS Fargate (2 vCPU, 4 GB) behind Application Load Balancer, auto-scale 2-10 tasks.\n",
        "    - S3 static front-end optional; Streamlit runs inside container on 8501.\n",
        "    - CloudWatch logs + alarm on p95 latency > 1 s.\n",
        "\n",
        "12. Post-deployment surveillance\n",
        "    - Log every inference request (no image, only hash & score) → S3 + Athena.\n",
        "    - Weekly recompute drift metrics (PSI, KL) on new uploads; trigger retrain if PSI > 0.2.\n",
        "    - Maintain model registry (MLflow) with version tags tied to container image SHA.\n",
        "\n",
        "13. User training & disclaimers\n",
        "   - Landing page shows “Not for primary diagnosis – physician must verify”.\n",
        "   - Provide calibration curve and failure-case examples.\n",
        "   - Include “Report adverse result” button → creates Jira ticket for safety team.\n",
        "\n",
        "Outcome: clinicians drag-and-drop an X-ray in browser → prediction + heat-map in < 1 s, zero installation, full audit trail."
      ],
      "metadata": {
        "id": "CmKmwgsoKFPk"
      }
    }
  ]
}